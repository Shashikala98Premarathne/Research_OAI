{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '/Users/shashinimashi/Desktop/Semester 3/Thesis/Analysis/Repo/Research_OAI/Research_OAI/transcripts.csv'  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check if the 'Transcription' column exists\n",
    "if 'Transcription' not in df.columns:\n",
    "    raise ValueError(\"The 'Transcription' column is missing in the dataset.\")\n",
    "\n",
    "# Load the pretrained embedding model\n",
    "embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Generate embeddings for the Transcription column\n",
    "df['Transcription_Embeddings'] = df['Transcription'].apply(\n",
    "    lambda text: embedding_model.encode(text, convert_to_tensor=True)\n",
    ")\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "output_path = 'transcription_embeddings.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Embeddings file saved to {output_path}\")\n",
    "\n",
    "\n",
    "#open with pickel\n",
    "# Load train response embeddings\n",
    "#with open('train_response_embeddings.pkl', 'rb') as f:\n",
    "    loaded_train_response_embeddings = pickle.load(f)\n",
    "\n",
    "# Load validation response embeddings\n",
    "#with open('val_response_embeddings.pkl', 'rb') as f:\n",
    "    loaded_val_response_embeddings = pickle.load(f)\n",
    "\n",
    "# Load test response embeddings\n",
    "#with open('test_response_embeddings.pkl', 'rb') as f:\n",
    "#   loaded_test_response_embeddings = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(df_cleaned, test_size=0.2, random_state=42)  # 80% training data\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # 10% validation, 10% test\n",
    "\n",
    "print(f\"Training Set: {len(train_df)}\")\n",
    "print(f\"Validation Set: {len(val_df)}\")\n",
    "print(f\"Test Set: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column names of each dataframe\n",
    "print(f\"train_df columns: {train_df.columns}\")\n",
    "print(f\"val_df columns: {val_df.columns}\")\n",
    "print(f\"test_df columns: {test_df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows of 'Responses' column in each dataset\n",
    "print(train_df['Responses'].head())\n",
    "print(val_df['Responses'].head())\n",
    "print(test_df['Responses'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many valid responses are left\n",
    "print(f\"Valid Train Responses: {len([r for r in train_responses if r != ''])}\")\n",
    "print(f\"Valid Validation Responses: {len([r for r in val_responses if r != ''])}\")\n",
    "print(f\"Valid Test Responses: {len([r for r in test_responses if r != ''])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few responses after cleaning\n",
    "print(train_df['Responses'].head())\n",
    "print(val_df['Responses'].head())\n",
    "print(test_df['Responses'].head())\n",
    "\n",
    "# Check if there are any empty or None values\n",
    "empty_train_responses = train_df['Responses'].apply(lambda x: len(x) == 0 if isinstance(x, list) else True)\n",
    "empty_val_responses = val_df['Responses'].apply(lambda x: len(x) == 0 if isinstance(x, list) else True)\n",
    "empty_test_responses = test_df['Responses'].apply(lambda x: len(x) == 0 if isinstance(x, list) else True)\n",
    "\n",
    "print(f\"Empty Train Responses: {empty_train_responses.sum()}\")\n",
    "print(f\"Empty Validation Responses: {empty_val_responses.sum()}\")\n",
    "print(f\"Empty Test Responses: {empty_test_responses.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the SentenceTransformer model (all-mpnet-base-v2)\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Function to generate sentence embeddings\n",
    "def get_sentence_embeddings(text_list):\n",
    "    # Generate sentence embeddings for the text list\n",
    "    return model.encode(text_list, convert_to_tensor=True)\n",
    "\n",
    "# Flatten the list of sentences (Responses are lists of sentences, so flatten them into one list)\n",
    "def flatten_responses(responses):\n",
    "    return [sentence for response in responses for sentence in response]\n",
    "\n",
    "# For the training, validation, and test sets, flatten the responses before getting embeddings\n",
    "train_flat_responses = flatten_responses(train_df['Responses'].tolist())\n",
    "val_flat_responses = flatten_responses(val_df['Responses'].tolist())\n",
    "test_flat_responses = flatten_responses(test_df['Responses'].tolist())\n",
    "\n",
    "# Get embeddings for the flattened responses\n",
    "train_embeddings = get_sentence_embeddings(train_flat_responses)\n",
    "val_embeddings = get_sentence_embeddings(val_flat_responses)\n",
    "test_embeddings = get_sentence_embeddings(test_flat_responses)\n",
    "\n",
    "# Now, group by ResponseID (Mean Pooling) - assuming 'ResponseID' is already available\n",
    "# Grouping the responses by ResponseID and applying mean pooling to get one embedding per response\n",
    "def group_by_response_id(df, embeddings):\n",
    "    response_embeddings = {}\n",
    "    \n",
    "    # Iterate through unique ResponseIDs\n",
    "    for response_id in df['ResponseID'].unique():\n",
    "        # Get sentences for the given ResponseID\n",
    "        sentences = df[df['ResponseID'] == response_id]['Responses'].tolist()\n",
    "        \n",
    "        # Flatten the sentences for the ResponseID\n",
    "        sentences_flat = flatten_responses(sentences)\n",
    "        \n",
    "        # Generate sentence embeddings for the flattened sentences\n",
    "        sentences_embeddings = model.encode(sentences_flat, convert_to_tensor=True)\n",
    "        \n",
    "        # Apply mean pooling using PyTorch's .mean() method along the right axis\n",
    "        response_embeddings[response_id] = sentences_embeddings.mean(dim=0)  # Mean Pooling\n",
    "    \n",
    "    return response_embeddings\n",
    "\n",
    "# Example for training, validation, and test sets\n",
    "train_response_embeddings = group_by_response_id(train_df, train_embeddings)\n",
    "val_response_embeddings = group_by_response_id(val_df, val_embeddings)\n",
    "test_response_embeddings = group_by_response_id(test_df, test_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the SentenceTransformer model (all-mpnet-base-v2)\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def calculate_cosine_similarity(query_embedding, response_embeddings):\n",
    "    similarities = {}\n",
    "    for response_id, embedding in response_embeddings.items():\n",
    "        # Ensure both embeddings are moved to CPU and converted to NumPy\n",
    "        similarity = cosine_similarity(\n",
    "            query_embedding.cpu().numpy(),\n",
    "            embedding.cpu().numpy().reshape(1, -1)\n",
    "        )[0][0]\n",
    "        similarities[response_id] = similarity\n",
    "    return similarities\n",
    "\n",
    "# Example threshold and fallback count\n",
    "THRESHOLD = 0.2\n",
    "TOP_N = 5  # Fallback: Select top N responses if no responses pass the threshold\n",
    "\n",
    "# Normalize keyword embeddings\n",
    "keyword_embeddings_cpu = normalize(keyword_embeddings.cpu().numpy(), axis=1)\n",
    "\n",
    "# Calculate cosine similarity between keyword embeddings and response embeddings\n",
    "response_similarities = {}  # Store similarities for all responses\n",
    "for response_id, embedding in train_response_embeddings.items():\n",
    "    # Normalize response embeddings\n",
    "    response_embedding = normalize(embedding.cpu().numpy().reshape(1, -1))\n",
    "    \n",
    "    # Calculate cosine similarity for all keywords\n",
    "    max_similarity = max(\n",
    "        cosine_similarity(keyword_embeddings_cpu, response_embedding).flatten()\n",
    "    )\n",
    "    response_similarities[response_id] = max_similarity\n",
    "\n",
    "# Filter responses based on threshold\n",
    "filtered_responses = {\n",
    "    response_id: sim for response_id, sim in response_similarities.items() if sim >= THRESHOLD\n",
    "}\n",
    "\n",
    "# Fallback: If no responses pass the threshold, select the top N most similar responses\n",
    "if not filtered_responses:\n",
    "    print(\"\\nNo responses passed the threshold. Using fallback mechanism.\")\n",
    "    filtered_responses = dict(sorted(response_similarities.items(), key=lambda x: x[1], reverse=True)[:TOP_N])\n",
    "\n",
    "# Display filtered responses\n",
    "print(\"\\nFiltered Responses:\")\n",
    "print(filtered_responses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the SentenceTransformer model (all-mpnet-base-v2)\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Function to generate sentence embeddings\n",
    "def get_sentence_embeddings(text_list):\n",
    "    # Generate sentence embeddings for the text list\n",
    "    return model.encode(text_list, convert_to_tensor=True)\n",
    "\n",
    "# Flatten the list of sentences (Responses are lists of sentences, so flatten them into one list)\n",
    "def flatten_responses(responses):\n",
    "    return [sentence for response in responses for sentence in response]\n",
    "\n",
    "# For the training, validation, and test sets, flatten the responses before getting embeddings\n",
    "train_flat_responses = flatten_responses(train_df['Responses'].tolist())\n",
    "val_flat_responses = flatten_responses(val_df['Responses'].tolist())\n",
    "test_flat_responses = flatten_responses(test_df['Responses'].tolist())\n",
    "\n",
    "# Get embeddings for the flattened responses\n",
    "train_embeddings = get_sentence_embeddings(train_flat_responses)\n",
    "val_embeddings = get_sentence_embeddings(val_flat_responses)\n",
    "test_embeddings = get_sentence_embeddings(test_flat_responses)\n",
    "\n",
    "# Now, group by ResponseID (Mean Pooling) - assuming 'ResponseID' is already available\n",
    "# Grouping the responses by ResponseID and applying mean pooling to get one embedding per response\n",
    "def group_by_response_id(df, embeddings):\n",
    "    response_embeddings = {}\n",
    "    \n",
    "    # Iterate through unique ResponseIDs\n",
    "    for response_id in df['ResponseID'].unique():\n",
    "        # Get sentences for the given ResponseID\n",
    "        sentences = df[df['ResponseID'] == response_id]['Responses'].tolist()\n",
    "        \n",
    "        # Flatten the sentences for the ResponseID\n",
    "        sentences_flat = flatten_responses(sentences)\n",
    "        \n",
    "        # Generate sentence embeddings for the flattened sentences\n",
    "        sentences_embeddings = model.encode(sentences_flat, convert_to_tensor=True)\n",
    "        \n",
    "        # Apply mean pooling using PyTorch's .mean() method along the right axis\n",
    "        response_embeddings[response_id] = sentences_embeddings.mean(dim=0)  # Mean Pooling\n",
    "    \n",
    "    return response_embeddings\n",
    "\n",
    "# Example for training, validation, and test sets\n",
    "train_response_embeddings = group_by_response_id(train_df, train_embeddings)\n",
    "val_response_embeddings = group_by_response_id(val_df, val_embeddings)\n",
    "test_response_embeddings = group_by_response_id(test_df, test_embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
