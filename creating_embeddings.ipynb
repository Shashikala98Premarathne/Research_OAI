{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '/Users/shashinimashi/Desktop/Semester 3/Thesis/Analysis/Repo/Research_OAI/Research_OAI/transcripts.csv'  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check if the 'Transcription' column exists\n",
    "if 'Transcription' not in df.columns:\n",
    "    raise ValueError(\"The 'Transcription' column is missing in the dataset.\")\n",
    "\n",
    "# Load the pretrained embedding model\n",
    "embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Generate embeddings for the Transcription column\n",
    "df['Transcription_Embeddings'] = df['Transcription'].apply(\n",
    "    lambda text: embedding_model.encode(text, convert_to_tensor=True)\n",
    ")\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "output_path = 'transcription_embeddings.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Embeddings file saved to {output_path}\")\n",
    "\n",
    "\n",
    "#open with pickel\n",
    "# Load train response embeddings\n",
    "#with open('train_response_embeddings.pkl', 'rb') as f:\n",
    "    loaded_train_response_embeddings = pickle.load(f)\n",
    "\n",
    "# Load validation response embeddings\n",
    "#with open('val_response_embeddings.pkl', 'rb') as f:\n",
    "    loaded_val_response_embeddings = pickle.load(f)\n",
    "\n",
    "# Load test response embeddings\n",
    "#with open('test_response_embeddings.pkl', 'rb') as f:\n",
    "#   loaded_test_response_embeddings = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(df_cleaned, test_size=0.2, random_state=42)  # 80% training data\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # 10% validation, 10% test\n",
    "\n",
    "print(f\"Training Set: {len(train_df)}\")\n",
    "print(f\"Validation Set: {len(val_df)}\")\n",
    "print(f\"Test Set: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column names of each dataframe\n",
    "print(f\"train_df columns: {train_df.columns}\")\n",
    "print(f\"val_df columns: {val_df.columns}\")\n",
    "print(f\"test_df columns: {test_df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows of 'Responses' column in each dataset\n",
    "print(train_df['Responses'].head())\n",
    "print(val_df['Responses'].head())\n",
    "print(test_df['Responses'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many valid responses are left\n",
    "print(f\"Valid Train Responses: {len([r for r in train_responses if r != ''])}\")\n",
    "print(f\"Valid Validation Responses: {len([r for r in val_responses if r != ''])}\")\n",
    "print(f\"Valid Test Responses: {len([r for r in test_responses if r != ''])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few responses after cleaning\n",
    "print(train_df['Responses'].head())\n",
    "print(val_df['Responses'].head())\n",
    "print(test_df['Responses'].head())\n",
    "\n",
    "# Check if there are any empty or None values\n",
    "empty_train_responses = train_df['Responses'].apply(lambda x: len(x) == 0 if isinstance(x, list) else True)\n",
    "empty_val_responses = val_df['Responses'].apply(lambda x: len(x) == 0 if isinstance(x, list) else True)\n",
    "empty_test_responses = test_df['Responses'].apply(lambda x: len(x) == 0 if isinstance(x, list) else True)\n",
    "\n",
    "print(f\"Empty Train Responses: {empty_train_responses.sum()}\")\n",
    "print(f\"Empty Validation Responses: {empty_val_responses.sum()}\")\n",
    "print(f\"Empty Test Responses: {empty_test_responses.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the SentenceTransformer model (all-mpnet-base-v2)\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Function to generate sentence embeddings\n",
    "def get_sentence_embeddings(text_list):\n",
    "    # Generate sentence embeddings for the text list\n",
    "    return model.encode(text_list, convert_to_tensor=True)\n",
    "\n",
    "# Flatten the list of sentences (Responses are lists of sentences, so flatten them into one list)\n",
    "def flatten_responses(responses):\n",
    "    return [sentence for response in responses for sentence in response]\n",
    "\n",
    "# For the training, validation, and test sets, flatten the responses before getting embeddings\n",
    "train_flat_responses = flatten_responses(train_df['Responses'].tolist())\n",
    "val_flat_responses = flatten_responses(val_df['Responses'].tolist())\n",
    "test_flat_responses = flatten_responses(test_df['Responses'].tolist())\n",
    "\n",
    "# Get embeddings for the flattened responses\n",
    "train_embeddings = get_sentence_embeddings(train_flat_responses)\n",
    "val_embeddings = get_sentence_embeddings(val_flat_responses)\n",
    "test_embeddings = get_sentence_embeddings(test_flat_responses)\n",
    "\n",
    "# Now, group by ResponseID (Mean Pooling) - assuming 'ResponseID' is already available\n",
    "# Grouping the responses by ResponseID and applying mean pooling to get one embedding per response\n",
    "def group_by_response_id(df, embeddings):\n",
    "    response_embeddings = {}\n",
    "    \n",
    "    # Iterate through unique ResponseIDs\n",
    "    for response_id in df['ResponseID'].unique():\n",
    "        # Get sentences for the given ResponseID\n",
    "        sentences = df[df['ResponseID'] == response_id]['Responses'].tolist()\n",
    "        \n",
    "        # Flatten the sentences for the ResponseID\n",
    "        sentences_flat = flatten_responses(sentences)\n",
    "        \n",
    "        # Generate sentence embeddings for the flattened sentences\n",
    "        sentences_embeddings = model.encode(sentences_flat, convert_to_tensor=True)\n",
    "        \n",
    "        # Apply mean pooling using PyTorch's .mean() method along the right axis\n",
    "        response_embeddings[response_id] = sentences_embeddings.mean(dim=0)  # Mean Pooling\n",
    "    \n",
    "    return response_embeddings\n",
    "\n",
    "# Example for training, validation, and test sets\n",
    "train_response_embeddings = group_by_response_id(train_df, train_embeddings)\n",
    "val_response_embeddings = group_by_response_id(val_df, val_embeddings)\n",
    "test_response_embeddings = group_by_response_id(test_df, test_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'keyword_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     23\u001b[0m TOP_N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Fallback: Select top N responses if no responses pass the threshold\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Train Set Embeddings\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Assuming `train_response_embeddings` contains grouped embeddings from the train set\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Assuming `keyword_embeddings` contains embeddings for input keywords\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m keyword_embeddings_cpu \u001b[38;5;241m=\u001b[39m normalize(\u001b[43mkeyword_embeddings\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Calculate cosine similarity between keyword embeddings and train response embeddings\u001b[39;00m\n\u001b[1;32m     31\u001b[0m train_response_similarities \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# Store similarities for training responses\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keyword_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "# Example threshold and fallback count\n",
    "THRESHOLD = 0.2\n",
    "TOP_N = 5  # Fallback: Select top N responses if no responses pass the threshold\n",
    "\n",
    "# Normalize keyword embeddings\n",
    "keyword_embeddings_cpu = normalize(keyword_embeddings.cpu().numpy(), axis=1)\n",
    "\n",
    "# Calculate cosine similarity between keyword embeddings and response embeddings\n",
    "response_similarities = {}  # Store similarities for all responses\n",
    "for response_id, embedding in train_response_embeddings.items():\n",
    "    # Normalize response embeddings\n",
    "    response_embedding = normalize(embedding.cpu().numpy().reshape(1, -1))\n",
    "    \n",
    "    # Calculate cosine similarity for all keywords\n",
    "    max_similarity = max(\n",
    "        cosine_similarity(keyword_embeddings_cpu, response_embedding).flatten()\n",
    "    )\n",
    "    response_similarities[response_id] = max_similarity\n",
    "\n",
    "# Filter responses based on threshold\n",
    "filtered_responses = {\n",
    "    response_id: sim for response_id, sim in response_similarities.items() if sim >= THRESHOLD\n",
    "}\n",
    "\n",
    "# Fallback: If no responses pass the threshold, select the top N most similar responses\n",
    "if not filtered_responses:\n",
    "    print(\"\\nNo responses passed the threshold. Using fallback mechanism.\")\n",
    "    filtered_responses = dict(sorted(response_similarities.items(), key=lambda x: x[1], reverse=True)[:TOP_N])\n",
    "\n",
    "# Display filtered responses\n",
    "print(\"\\nFiltered Responses:\")\n",
    "print(filtered_responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the SentenceTransformer model (all-mpnet-base-v2)\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Function to generate sentence embeddings\n",
    "def get_sentence_embeddings(text_list):\n",
    "    # Generate sentence embeddings for the text list\n",
    "    return model.encode(text_list, convert_to_tensor=True)\n",
    "\n",
    "# Flatten the list of sentences (Responses are lists of sentences, so flatten them into one list)\n",
    "def flatten_responses(responses):\n",
    "    return [sentence for response in responses for sentence in response]\n",
    "\n",
    "# For the training, validation, and test sets, flatten the responses before getting embeddings\n",
    "train_flat_responses = flatten_responses(train_df['Responses'].tolist())\n",
    "val_flat_responses = flatten_responses(val_df['Responses'].tolist())\n",
    "test_flat_responses = flatten_responses(test_df['Responses'].tolist())\n",
    "\n",
    "# Get embeddings for the flattened responses\n",
    "train_embeddings = get_sentence_embeddings(train_flat_responses)\n",
    "val_embeddings = get_sentence_embeddings(val_flat_responses)\n",
    "test_embeddings = get_sentence_embeddings(test_flat_responses)\n",
    "\n",
    "# Now, group by ResponseID (Mean Pooling) - assuming 'ResponseID' is already available\n",
    "# Grouping the responses by ResponseID and applying mean pooling to get one embedding per response\n",
    "def group_by_response_id(df, embeddings):\n",
    "    response_embeddings = {}\n",
    "    \n",
    "    # Iterate through unique ResponseIDs\n",
    "    for response_id in df['ResponseID'].unique():\n",
    "        # Get sentences for the given ResponseID\n",
    "        sentences = df[df['ResponseID'] == response_id]['Responses'].tolist()\n",
    "        \n",
    "        # Flatten the sentences for the ResponseID\n",
    "        sentences_flat = flatten_responses(sentences)\n",
    "        \n",
    "        # Generate sentence embeddings for the flattened sentences\n",
    "        sentences_embeddings = model.encode(sentences_flat, convert_to_tensor=True)\n",
    "        \n",
    "        # Apply mean pooling using PyTorch's .mean() method along the right axis\n",
    "        response_embeddings[response_id] = sentences_embeddings.mean(dim=0)  # Mean Pooling\n",
    "    \n",
    "    return response_embeddings\n",
    "\n",
    "# Example for training, validation, and test sets\n",
    "train_response_embeddings = group_by_response_id(train_df, train_embeddings)\n",
    "val_response_embeddings = group_by_response_id(val_df, val_embeddings)\n",
    "test_response_embeddings = group_by_response_id(test_df, test_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the filtered responses' text\n",
    "filtered_texts = [train_df.loc[train_df['ResponseID'] == resp_id, 'Transcription'].values[0]\n",
    "                  for resp_id in filtered_responses.keys()]\n",
    "\n",
    "# If no responses are found (extremely unlikely with the fallback), handle gracefully\n",
    "if not filtered_texts:\n",
    "    filtered_texts = [\"No relevant responses found.\"]\n",
    "\n",
    "# Prepare input for T5\n",
    "input_text = \"summarize: \" + \" \".join(filtered_texts)\n",
    "input_ids = t5_tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "# Generate summary with T5\n",
    "summary_ids = t5_model.generate(input_ids, max_length=150, num_beams=5, early_stopping=True)\n",
    "summary = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Display the generated summary\n",
    "print(\"\\nGenerated Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Ensure `generated_summaries` and `responses` are aligned\n",
    "# `generated_summaries` is the list of summaries produced by T5\n",
    "# `responses` is the list of original responses (ground truth)\n",
    "generated_summaries = [\"Generated summary 1\", \"Generated summary 2\", \"Generated summary 3\"]\n",
    "responses = [\"Ground truth response 1\", \"Ground truth response 2\", \"Ground truth response 3\"]\n",
    "\n",
    "# Initialize scores\n",
    "rouge_1_scores = []\n",
    "rouge_l_scores = []\n",
    "bleu_scores = []\n",
    "\n",
    "# Evaluate each pair of generated summary and ground truth\n",
    "for gen_summary, response in zip(generated_summaries, responses):\n",
    "    # Compute ROUGE scores\n",
    "    rouge_scores = rouge_scorer.score(response, gen_summary)\n",
    "    rouge_1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "    rouge_l_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_score = sentence_bleu(\n",
    "        [response.split()],  # Ground truth tokenized\n",
    "        gen_summary.split(),  # Generated summary tokenized\n",
    "        smoothing_function=SmoothingFunction().method1  # Smoothing to handle brevity\n",
    "    )\n",
    "    bleu_scores.append(bleu_score)\n",
    "\n",
    "# Calculate average scores\n",
    "avg_rouge_1 = sum(rouge_1_scores) / len(rouge_1_scores)\n",
    "avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n",
    "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "# Print results\n",
    "print(f\"Average ROUGE-1: {avg_rouge_1:.4f}\")\n",
    "print(f\"Average ROUGE-L: {avg_rouge_l:.4f}\")\n",
    "print(f\"Average BLEU: {avg_bleu:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
